model_name: slu
slots:                # Just an example, please change this to your own slots.
  _cancel_:           # Intent name
    number_slot:      # slot name
    - number          # entity types
timerange_constraints:
  time:
    lte:
      hour: 23
      minute: 59
    gte:
      hour: 0
      minute: 0
tasks:
  classification:
    confidence_levels: []
    alias: {}
    format: ''
    model_args:
      production:
        best_model_dir: null
        dynamic_quantize: true
        eval_batch_size: 10                                    # same as the number of inputs going into the model.
        max_seq_length: 128                                    # reduce this to get less latency in exchange of accuracy.
        no_cache: true
        output_dir: null
        reprocess_input_data: true
        silent: true
        thread_count: 1
        use_multiprocessing: false                            # Setting this to true hurts performance!
        use_multiprocessing_for_evaluation: false
      test:
        best_model_dir: null
        output_dir: null
        reprocess_input_data: true
        silent: true
        use_multiprocessing: false                            # Setting this to true hurts performance!
        use_multiprocessing_for_evaluation: false
      train:
        best_model_dir: null
        early_stopping_consider_epochs: true
        early_stopping_delta: 0.01
        early_stopping_metric: eval_loss
        early_stopping_metric_minimize: true
        early_stopping_patience: 3
        eval_batch_size: 8
        fp16: false
        gridsearch_hyperparams:
          use: true                            # bool indicating usage of gridsearch for hyperparameter tuning if using MLPMultiClass plugin
          verbose_level: 2                     # GridSearchCV verbosity from sklearn docs
          cv: 2                                # Cross validation split
          params:                              # All params name, definitions and possible values according to sklearn tfidfvectorizer and mlpclassifier docs
            activation:
            - relu
            - tanh
            alpha:
            - 0.0001
            - 0.005
            batch_size:
            - auto
            early_stopping:
            - false
            - true
            hidden_layer_sizes:
            - (100, )
            - (200, 10, 2)
            - (40, 20)
            learning_rate:
            - constant
            max_iter:
            - 25
            - 40
            ngram_range:
            - (1, 1)
            solver:
            - adam
        num_train_epochs: 1
        output_dir: null
        overwrite_output_dir: true
        reprocess_input_data: true
        save_eval_checkpoints: false
        save_model_every_epoch: false
        save_steps: -1
        use_early_stopping: true
    skip:                                       # Skip the intents that are not required for training, will still be used for testing.
    - silence
    - audio_noisy
    - partial
    - _ood_
    - ood
    - _oos_
    - oos
    - audio_silent
    - audio_channel_noise
    - audio_speech_unclear
    - audio_speech_volume
    - background_speech
    - other_language
    threshold: 0.2
    use: true
calibration: {}
entity_patterns:
  fruits:
    red:
    - apple
datetime_rules:
  state_name:
    rewind:
      days: 60
languages:
- en
critical_intents: []
